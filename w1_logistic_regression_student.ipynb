{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SWT17/Bachelor_project/blob/main/w1_logistic_regression_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBsS1qZTtAZE"
      },
      "source": [
        "#load packages\n",
        "import numpy as np\n",
        "import sklearn.model_selection\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfhsm8m6tFe_"
      },
      "source": [
        "#make helper functions\n",
        "\n",
        "def plot_digit(X, y, idx):\n",
        "    img = X[idx].reshape(28,28)\n",
        "    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
        "    plt.title('true label: %d' % y[idx])\n",
        "    plt.show()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30OCX2pjwGdT"
      },
      "source": [
        "#load data\n",
        "\n",
        "from mlxtend.data import mnist_data\n",
        "X, y = mnist_data()\n",
        "\n",
        "\n",
        "#we just focus on 1's and 0's:\n",
        "keep=(y==0) | (y==1)\n",
        "X=X[keep,:]\n",
        "y=y[keep]\n",
        "\n",
        "y=np.expand_dims(y,1)\n",
        "\n",
        "#y = y[0:3]\n",
        "#X=X[0:3]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR1s1Q-twJ7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "bf7544fb-5f6d-4bd5-af6e-b344a8446660"
      },
      "source": [
        "#inspect data\n",
        "print(len(y))\n",
        "#(plot digit)\n",
        "(print(X[2].shape))\n",
        "plot_digit(X, y,2)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "(784,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cg7bDdAwSsl"
      },
      "source": [
        "# create a loss function:\n",
        "def avrLoss(y,a):\n",
        "\n",
        "  #INSERT CODE\n",
        "    import numpy as np\n",
        "    epsilon = 1e-10  # Small constant to avoid log(0) issues\n",
        "    batch_size = len(y)\n",
        "    losses = []\n",
        "\n",
        "    # Calculate the individual losses for each example in the batch\n",
        "    for i in range(batch_size):\n",
        "      losses.append(-(y[i] * np.log(a[i] + epsilon) + (1 - y[i]) * np.log(1 - a[i] + epsilon)))\n",
        "\n",
        "    # Calculate the average loss for the batch\n",
        "    avg_loss = np.mean(losses[:])\n",
        "\n",
        "    return avg_loss"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlbHgUIjwXiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4adc83e-64b8-4756-9703-211d94c29e45"
      },
      "source": [
        "#test loss function:\n",
        "#(just run this cell and see what happens)\n",
        "\n",
        "loss=avrLoss(np.array([0,1,0,1]),np.linspace(.1,.9,4))\n",
        "print(loss)\n",
        "assert np.all(np.around(loss,9)==0.554331312)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5543313120688863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdCmhY63weJO"
      },
      "source": [
        "#create a sigmoid\n",
        "def sigmoid(z):\n",
        "  #INSERT CODE\n",
        "  import math\n",
        "  if z > 101:\n",
        "    z = 100\n",
        "  if z <-100:\n",
        "    z = -100\n",
        "\n",
        "  val = 1 / (1 + np.exp(-z))\n",
        "  #print(val)\n",
        "  return val\n"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W17O55pwiIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c53c738-391d-4248-d39c-7f83676a7c1e"
      },
      "source": [
        "#check sigmoid\n",
        "#(just run this cell and see what happens)\n",
        "out=sigmoid(np.array(range(-5,5)))\n",
        "print(out)\n",
        "assert np.all(np.around(out,8)==np.array([0.00669285, 0.01798621, 0.04742587, 0.11920292, 0.26894142, 0.5, 0.73105858, 0.88079708, 0.95257413, 0.98201379]))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00669285 0.01798621 0.04742587 0.11920292 0.26894142 0.5\n",
            " 0.73105858 0.88079708 0.95257413 0.98201379]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhAlECUCwh8r"
      },
      "source": [
        "#make test & train data\n",
        "\n",
        "Xtrain,Xtest,ytrain,ytest=sklearn.model_selection.train_test_split(X,y,test_size=0.3,shuffle=True)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FINAL VERSION\n",
        "#data format:\n",
        "_ytrain = ytrain[:,0]\n",
        "_ytest = ytest[:,0]\n",
        "\n",
        "#Traning values:\n",
        "nEpoch=8\n",
        "trainRate=0.00001 #pick a number less than 1. Any bigger and it converts in first Epoc\n",
        "accuracy=np.zeros(nEpoch)\n",
        "\n",
        "#INITIATE WEIGHTS AND BIASES (w,b)\n",
        "b = 0.5\n",
        "w = np.random.random(len(Xtrain[1]))\n",
        "\n",
        "#Batches:\n",
        "bs = 100 #100 batches and 7 images in each batch\n",
        "Xbatches = np.array_split(Xtrain[:], bs)\n",
        "ybatches = np.array_split(ytrain[:], bs)\n",
        "\n",
        "\n",
        "for iTrain in range(nEpoch):\n",
        "  for j in range(bs): #7 images in each batch\n",
        "      Xbatch = Xbatches[j]\n",
        "      ybatch = ybatches[j]\n",
        "      yval = ybatch[:,0].tolist()\n",
        "\n",
        "      a_ = []\n",
        "      for Xval in Xbatch:\n",
        "        z = np.matmul(w,Xval)+b\n",
        "        a = sigmoid(z)\n",
        "        a_.append(a)\n",
        "\n",
        "      #update weights\n",
        "      wu = []\n",
        "      for u in range(7):\n",
        "        wgrad = (a_[u]-yval[u])*Xbatch[u]\n",
        "        wu.append(wgrad)\n",
        "        w = w - trainRate*wgrad\n",
        "\n",
        "        bgrad = a_[u]-yval[u]\n",
        "        b = b - trainRate*bgrad\n",
        "\n",
        "      #w = w - np.mean(wu, axis = 0)\n",
        "\n",
        "      #calculate loss & accuracy\n",
        "      loss = avrLoss(yval,a_)\n",
        "      yhat=a>.5\n",
        "      accuracy[iTrain]=np.mean(yhat==ytrain)\n",
        "\n",
        "  if np.mod(iTrain,1)==0:\n",
        "    print()\n",
        "    print(\"Epoc: \", iTrain)\n",
        "    print(\"loss: \", loss)\n",
        "    print(\"accuracy: \", accuracy[iTrain])\n",
        "\n",
        "    ytemp = sigmoid(np.matmul(Xtrain,w)+b)\n",
        "    _Y_prediction_train = ytemp[:]\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(_Y_prediction_train - _ytrain)) * 100))\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"________________________________________\")\n",
        "print()\n",
        "print(\"Traning complete\")\n",
        "print(\"________________________________________\")\n",
        "print()\n",
        "\n",
        "print(\"Number of Epochs: \", nEpoch)\n",
        "print(\"TrainRate: \", trainRate)\n",
        "print(\"Parameters: \", len(w)+ 1) #1 for bias\n",
        "print()\n",
        "\n",
        "ytemp = sigmoid(np.matmul(Xtrain,w)+b)\n",
        "_Y_prediction_train = ytemp[:]\n",
        "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(_Y_prediction_train - _ytrain)) * 100))\n",
        "\n",
        "_Y_prediction_test = sigmoid(np.matmul(Xtest,w)+b)\n",
        "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(_Y_prediction_test - _ytest)) * 100))\n",
        "\n",
        "print()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKYru4wHpS7S",
        "outputId": "363678a1-20e0-402f-8ec8-b981957b479b"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoc:  0\n",
            "loss:  9.868221827060196\n",
            "accuracy:  0.49\n",
            "train accuracy: 64.71463367963565 %\n",
            "\n",
            "Epoc:  1\n",
            "loss:  -1.000000082690371e-10\n",
            "accuracy:  0.51\n",
            "train accuracy: 94.85714285714286 %\n",
            "\n",
            "Epoc:  2\n",
            "loss:  -1.000000082690371e-10\n",
            "accuracy:  0.51\n",
            "train accuracy: 96.73484437238892 %\n",
            "\n",
            "Epoc:  3\n",
            "loss:  -1.000000082690371e-10\n",
            "accuracy:  0.51\n",
            "train accuracy: 98.42857233707596 %\n",
            "\n",
            "Epoc:  4\n",
            "loss:  -1.000000082690371e-10\n",
            "accuracy:  0.51\n",
            "train accuracy: 98.7142857142857 %\n",
            "\n",
            "Epoc:  5\n",
            "loss:  -1.000000082690371e-10\n",
            "accuracy:  0.51\n",
            "train accuracy: 98.85667413104794 %\n",
            "\n",
            "Epoc:  6\n",
            "loss:  -1.000000082690371e-10\n",
            "accuracy:  0.51\n",
            "train accuracy: 99.0 %\n",
            "\n",
            "Epoc:  7\n",
            "loss:  -1.000000082690371e-10\n",
            "accuracy:  0.51\n",
            "train accuracy: 99.14285714285472 %\n",
            "\n",
            "________________________________________\n",
            "\n",
            "Traning complete\n",
            "________________________________________\n",
            "\n",
            "Number of Epochs:  8\n",
            "TrainRate:  1e-05\n",
            "Parameters:  785\n",
            "\n",
            "train accuracy: 99.14285714285472 %\n",
            "test accuracy: 99.0 %\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-158-7eb9579ba5a3>:10: RuntimeWarning: overflow encountered in exp\n",
            "  val = 1 / (1 + np.exp(-z))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#multi image batch...\n",
        "nEpoch=25\n",
        "trainRate=0.00001 #pick a number less than 1\n",
        "accuracy=np.zeros(nEpoch)\n",
        "\n",
        "#INITIATE WEIGHTS AND BIASES (w,b)\n",
        "b = 0.25\n",
        "w = np.random.random(len(Xtrain[1]))\n",
        "#print(\"initial w:\", w[130:135])\n",
        "#print(\"initial sum w:\", sum(w[130:135]))\n",
        "\n",
        "#batches:\n",
        "bs = 100 #100 batches and 7 images in each batch\n",
        "Xbatches = np.array_split(Xtrain[:], bs)\n",
        "ybatches = np.array_split(ytrain[:], bs)\n",
        "\n",
        "for iTrain in range(nEpoch):\n",
        "  for j in range(bs): #7 images in each batch\n",
        "      Xbatch = Xbatches[j]\n",
        "      ybatch = ybatches[j]\n",
        "      yval = ybatch[:,0].tolist()\n",
        "      # Vi har nu 7 X-sÃ¦t\n",
        "\n",
        "      a_ = []\n",
        "      for Xval in Xbatch:\n",
        "        z = np.matmul(w,Xval)+b\n",
        "        #print(z)\n",
        "        a = sigmoid(z)\n",
        "        a_.append(a)\n",
        "\n",
        "      #update weights\n",
        "      wu = []\n",
        "      for u in range(7):\n",
        "        #print(\"yval: \", yval[u])\n",
        "        #print(\"a: \", a_[u])\n",
        "\n",
        "        #test = (a_[u]-yval[u])\n",
        "        #print(a_[u]-yval[u])\n",
        "        #print(Xbatch[u])\n",
        "        #print(test*Xbatch[u])\n",
        "        wgrad = (a_[u]-yval[u])*Xbatch[u]\n",
        "        wu.append(wgrad)\n",
        "        w = w - trainRate*wgrad\n",
        "        #print(sum(w))\n",
        "\n",
        "        bgrad = a_[u]-yval[u]\n",
        "        b = b - trainRate*bgrad\n",
        "\n",
        "      #w = w - np.mean(wu, axis = 0)\n",
        "\n",
        "      #calculate loss & accuracy\n",
        "      loss = avrLoss(yval,a_)\n",
        "      yhat=a>.5\n",
        "      accuracy[iTrain]=np.mean(yhat==ytrain)\n",
        "\n",
        "  if np.mod(iTrain,1)==0:\n",
        "    print(\"Epoc: \", iTrain)\n",
        "    #print(\"loss: \", loss)\n",
        "    #print(\"accuracy: \", accuracy[iTrain])\n",
        "    #print(\"w:\", w[130:135])\n",
        "\n",
        "    ytemp = sigmoid(np.matmul(Xtrain,w)+b)#>0.5\n",
        "    #q =700\n",
        "    #yp = Y_prediction_train[0:q].tolist()\n",
        "    _Y_prediction_train = ytemp[:]\n",
        "    #print(\"ypred: \", yp)\n",
        "\n",
        "    _ytrain = ytrain[:,0]\n",
        "    #print(\"ytrain: \", ytt)\n",
        "    #print(\"dif: \",np.mean(np.abs(yp - ytt)))\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(_Y_prediction_train - _ytrain)) * 100))\n",
        "    #print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - ytrain)) * 100))\n",
        "\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"________________________________________\")\n",
        "print()\n",
        "print(\"Traning complete\")\n",
        "print(\"________________________________________\")\n",
        "print()\n",
        "\n",
        "\n",
        "ytemp = sigmoid(np.matmul(Xtrain,w)+b)\n",
        "_Y_prediction_train = ytemp[:]\n",
        "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(_Y_prediction_train - _ytrain)) * 100))\n",
        "\n",
        "_ytest = ytest[:,0]\n",
        "_Y_prediction_test = sigmoid(np.matmul(Xtest,w)+b)\n",
        "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(_Y_prediction_test - _ytest)) * 100))\n",
        "\n",
        "print()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhg_tPDyxopA",
        "outputId": "43da041d-a417-48db-9e1d-0db208eb94bd"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-158-7eb9579ba5a3>:10: RuntimeWarning: overflow encountered in exp\n",
            "  val = 1 / (1 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoc:  0\n",
            "train accuracy: 69.72320505219443 %\n",
            "Epoc:  1\n",
            "train accuracy: 95.2857142857156 %\n",
            "Epoc:  2\n",
            "train accuracy: 96.94102407105143 %\n",
            "Epoc:  3\n",
            "train accuracy: 98.00000000352587 %\n",
            "Epoc:  4\n",
            "train accuracy: 98.00491033290028 %\n",
            "Epoc:  5\n",
            "train accuracy: 98.14285718026426 %\n",
            "Epoc:  6\n",
            "train accuracy: 98.42857710386161 %\n",
            "Epoc:  7\n",
            "train accuracy: 98.59172454008824 %\n",
            "Epoc:  8\n",
            "train accuracy: 98.71401528918551 %\n",
            "Epoc:  9\n",
            "train accuracy: 98.71436515771795 %\n",
            "Epoc:  10\n",
            "train accuracy: 99.05900381001476 %\n",
            "Epoc:  11\n",
            "train accuracy: 99.28546468879435 %\n",
            "Epoc:  12\n",
            "train accuracy: 99.38411914599806 %\n",
            "Epoc:  13\n",
            "train accuracy: 99.05750269049615 %\n",
            "Epoc:  14\n",
            "train accuracy: 99.42727413396379 %\n",
            "Epoc:  15\n",
            "train accuracy: 99.3503951441681 %\n",
            "Epoc:  16\n",
            "train accuracy: 99.42730083198124 %\n",
            "Epoc:  17\n",
            "train accuracy: 99.42913035197336 %\n",
            "Epoc:  18\n",
            "train accuracy: 99.45539946845551 %\n",
            "Epoc:  19\n",
            "train accuracy: 99.57019271966918 %\n",
            "Epoc:  20\n",
            "train accuracy: 99.68587271697882 %\n",
            "Epoc:  21\n",
            "train accuracy: 99.61079458227131 %\n",
            "Epoc:  22\n",
            "train accuracy: 99.7142857141653 %\n",
            "Epoc:  23\n",
            "train accuracy: 99.71428559126112 %\n",
            "Epoc:  24\n",
            "train accuracy: 99.7142894053045 %\n",
            "\n",
            "________________________________________\n",
            "\n",
            "Traning complete\n",
            "________________________________________\n",
            "\n",
            "train accuracy: 99.7142894053045 %\n",
            "test accuracy: 98.66666986763872 %\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nEpoch=5\n",
        "trainRate=0.1 #pick a number less than 1\n",
        "\n",
        "accuracy=np.zeros(nEpoch)\n",
        "\n",
        "#INITIATE WEIGHTS AND BIASES (w,b)\n",
        "w = np.random.random(len(Xtrain[1]))\n",
        "b = 0.0\n",
        "\n",
        "print(\"w:\", w[200:210])\n",
        "#Create one datastructure with all X, y and w values:\n",
        "#data = np.array([Xtrain[:], ytrain[:], w[:]])\n",
        "\n",
        "#batches:\n",
        "#batch_size = 784 / 56 #14\n",
        "bs = 784\n",
        "\n",
        "for iTrain in range(nEpoch):\n",
        "\n",
        "  for i in range(1): #700 all images\n",
        "    # Split the array into 56 batches with 14 pixels each\n",
        "    Xval =  Xtrain[i,:]\n",
        "    Xbatches = np.array_split(Xval[:], bs)\n",
        "\n",
        "    wbatches = np.array_split(w[:], bs)\n",
        "    yval = ytrain[i,0]\n",
        "\n",
        "    inx = 0\n",
        "\n",
        "    for j in range(bs): #56 all batches in each image\n",
        "      Xbatch = Xbatches[j]\n",
        "      wbatch = wbatches[j]\n",
        "\n",
        "      # Element-wise multiplication and sum\n",
        "      z = np.sum(wbatch * Xbatch)+b\n",
        "      #z = np.matmul(wbatch,Xbatch)+b\n",
        "      a = sigmoid(z)\n",
        "      #print(a)\n",
        "\n",
        "      #calculate loss & accuracy\n",
        "      loss = avrLoss(yval,a)\n",
        "\n",
        "      yhat=a>.5\n",
        "      accuracy[iTrain]=np.mean(yhat==ytrain)\n",
        "\n",
        "      #update weights\n",
        "      #wgrad = np.array([h * (a-yval)*trainRate for h in Xbatch])\n",
        "      wgrad = (a-yval)*trainRate*Xbatch[:]\n",
        "      #print(wgrad)\n",
        "      bgrad = a-yval\n",
        "\n",
        "      assert(wgrad.shape == wbatch.shape)\n",
        "\n",
        "      #UPDATE W AND b\n",
        "      ww = wbatch[:]*wgrad\n",
        "      #print(ww)\n",
        "      #print(wbatch[:])\n",
        "      #w[inx:inx+14] = ww\n",
        "      w[inx] = ww\n",
        "      #b = bgrad-trainRate*bgrad\n",
        "      #inx = inx + 14\n",
        "      inx = inx + 1\n",
        "\n",
        "  if np.mod(iTrain,1)==0:\n",
        "    print(\"Epoc: \", iTrain)\n",
        "    print(\"loss: \", loss)\n",
        "    print(\"accuracy: \", accuracy[iTrain])\n",
        "    #print(\"w: \", w[241])\n",
        "    print(\"w:\", w[200:210])\n",
        "\n",
        "\n",
        "print(\"w:\", w[200:210])\n",
        "Y_prediction_test = sigmoid(np.matmul(Xtest,w)+b)>0.5\n",
        "Y_prediction_train = sigmoid(np.matmul(Xtrain,w)+b)>0.5\n",
        "\n",
        "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - ytrain)) * 100))\n",
        "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - ytest)) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "8P4_YIFJJceq",
        "outputId": "e1a9befc-454e-4540-9963-f41c253e153a"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w: [0.8938284  0.6018951  0.6189382  0.09482611 0.90270949 0.91533046\n",
            " 0.02979275 0.6184958  0.94612316 0.66629531]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-c77fe2b8d3b3>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;31m#calculate loss & accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavrLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0myhat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-660f37f19c8b>\u001b[0m in \u001b[0;36mavrLoss\u001b[0;34m(y, a)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-10\u001b[0m  \u001b[0;31m# Small constant to avoid log(0) issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inx = 0\n",
        "w = np.random.random(5)\n",
        "print(w)\n",
        "\n",
        "ww = [3,3,3]\n",
        "w[inx:inx+3] = ww\n",
        "print(w)\n",
        "\n",
        "a = 0.6\n",
        "yval = 0\n",
        "www = [i * (a-yval) for i in ww]\n",
        "#www=(a-yval)*ww\n",
        "print(www)\n",
        "\n",
        "#print(ytrain[2])\n",
        "#print(yval)\n",
        "\n",
        "k = np.array([ww])\n",
        "kk = np.array([ww])\n",
        "b = 0\n",
        "g = k*kk\n",
        "print(g)\n",
        "z = np.sum(k * k)+b\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEB5ka7Yta-2",
        "outputId": "1c30fdf6-2f29-4594-a378-66c601e5957c"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.69652199 0.72034502 0.15367086 0.30941065 0.822279  ]\n",
            "[3.         3.         3.         0.30941065 0.822279  ]\n",
            "[1.7999999999999998, 1.7999999999999998, 1.7999999999999998]\n",
            "[[9 9 9]]\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.algorithms import diff\n",
        "#train\n",
        "\n",
        "#INITIATE WEIGHTS AND BIASES (w,b)\n",
        "w = np.random.random(len(Xtrain[1]))\n",
        "#print(len(w))\n",
        "b = 0\n",
        "#Please make w a vector with the same number of weights\n",
        "#as the number of pixels in each image, and make b a scalar.\n",
        "\n",
        "nEpoch=2\n",
        "Batch_size = 10\n",
        "\n",
        "accuracy=np.zeros(nEpoch)\n",
        "\n",
        "trainRate=0.05 #pick a number less than 1\n",
        "\n",
        "#batch\n",
        "batch_size = 784\n",
        "num_samples = len(Xtrain)\n",
        "num_pixels = 28*28\n",
        "\n",
        "# Calculate the number of batches\n",
        "#num_batches = num_samples // batch_size\n",
        "num_batches = 1\n",
        "\n",
        "# Reshape Xtrain to flatten the image dimensions\n",
        "Xtrain_flat = Xtrain.reshape(num_samples, -1)\n",
        "\n",
        "#print(ytrain[3][0])\n",
        "# Split Xtrain and ytrain into batches while maintaining pixel-image association\n",
        "\n",
        "#data with both x and y\n",
        "j = 0\n",
        "for j in range (3):\n",
        "  data = [Xtrain[j][:], ytrain[j][0]]\n",
        "\n",
        "\n",
        "arr = np.array([1, 2, 3])\n",
        "\n",
        "k = arr[:]\n",
        "#print(k)\n",
        "\n",
        "\n",
        "#batches:\n",
        "# Determine the batch size\n",
        "batch_size = 784 / 56 #14\n",
        "#batch number = 56\n",
        "\n",
        "kk = 0\n",
        "for image in Xtrain[:]:\n",
        "  #print(image)\n",
        "  if kk < 56:\n",
        "    # Split the array into 56 batches with 14 pixels each\n",
        "    batch = np.array_split(image[:], 56)\n",
        "    print(\"batch:\")\n",
        "    #print(batch[:][:]) # whole image\n",
        "    print(batch[:][23]) # middel of image\n",
        "\n",
        "    #need batch of weights... and the same for y-values....\n",
        "    kk = kk+1\n",
        "\n",
        "\n",
        "kk = 1\n",
        "if kk == 0:\n",
        "    # Split the array into 56 batches with 14 pixels each\n",
        "    Xbatch = np.array_split(Xval[:], 56)\n",
        "    print(\"Xbatch:\")\n",
        "    #print(batch[:][:]) # whole image\n",
        "    print(Xbatch[:][23]) # middel of image\n",
        "\n",
        "    print(\"ybatch:\")\n",
        "    ybatch = np.array_split(yval[:], 56)\n",
        "    print(ybatch[:][23]) # middel of image\n",
        "\n",
        "    print(\"wbatch:\")\n",
        "    wbatch = np.array_split(wval[:], 56)\n",
        "    print(wbatch[:][23]) # middel of image\n",
        "\n",
        "    #need batch of weights... and the same for y-values....\n",
        "\n",
        "    testX = data[0]\n",
        "    #b = testX[700,:]\n",
        "\n",
        "    testy = data[1]\n",
        "    bb = testy[:,0]\n",
        "\n",
        "    testw = data[2]\n",
        "    bbb = testw[:]\n",
        "\n",
        "    bbbb = np.array_split(bb, 56)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syGCA_NTzk9t",
        "outputId": "7a5c44a2-03d1-47e6-945e-dec1420071de"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch:\n",
            "[  0.   0.   0.   0.   0.   0. 138. 252. 183.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.  11. 183. 253. 253. 253.  37.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.  41. 226. 253. 253. 139.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.  68. 253. 253. 249. 131.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.   0. 174. 253. 191.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[ 62.  61.   0.   0.   0.   0.   0. 253. 252. 175.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[253. 250.  67.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[ 30. 215. 253. 253. 129.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[ 92. 254. 253. 234.  40.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[253. 253. 136.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.  25. 127. 252. 252. 120.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.  16.  24. 152. 253. 253. 253. 195.  28.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0. 117. 250.  99.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[149. 253. 252. 184.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[252. 253. 154.  12.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.  78. 219. 252. 237.  59.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.   0. 154. 254. 169.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0. 220. 253. 212.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[140. 253. 252.  55.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.   0.  22. 241. 254.  10.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.  16. 190. 253. 252. 252. 108.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[161. 253. 223.  16.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   3.   6.  51. 231. 254.  94.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[252. 252. 252. 173.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[255. 142.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[250. 222. 161.   0. 116. 253. 253. 253. 253. 253.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0. 206. 254. 254. 254.  15.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  2. 209. 253. 253. 111.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.   0.  91. 252. 252. 139.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[255. 255. 255.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[ 48.   0.   0.   0.   0.   0.  71. 236. 252. 252. 230.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.   0.  85. 251. 253.  83.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[253. 254. 254.  30.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[ 56. 231. 252. 252. 212.  35.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0. 109. 253.  91.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.   0.  46. 224. 253. 220.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[195.  38.   0.   0.   0.   0.   0.  33. 179. 253. 140.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   1.   6. 108. 254. 254. 210.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[ 35. 254. 254.  85.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  2.  88. 253. 253. 247. 109.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.  86. 252. 253. 176.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[251. 253. 251. 251. 251. 251. 253. 251. 251. 251.  71.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.  37. 252. 253. 252.  71.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.   0.  62. 253. 252.  71.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0.   0.   0. 176. 254. 230.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   7. 191. 252. 167.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[182.  18.   0.   0. 217. 253. 148.   0.  95.   5.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.  97. 250. 253. 253.  66.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[ 17. 198. 254. 217.  26.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[252. 231.  80.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[251. 253.  83.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0. 219. 197.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0. 205. 253.  32.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.  11. 167. 253. 248.  96.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[253. 252. 153.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "batch:\n",
            "[  0.   0.   0.   0.   0. 209. 253. 252. 227.  29.   0.   0.   0.   0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK_4dd1ywox5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "17d0ceee-d887-4513-b681-90261fc9ef13"
      },
      "source": [
        "from pandas.core.algorithms import diff\n",
        "#train\n",
        "\n",
        "#INITIATE WEIGHTS AND BIASES (w,b)\n",
        "w = np.random.random(len(Xtrain[1]))\n",
        "#print(len(w))\n",
        "b = 0\n",
        "#Please make w a vector with the same number of weights\n",
        "#as the number of pixels in each image, and make b a scalar.\n",
        "\n",
        "nEpoch=2\n",
        "Batch_size = 10\n",
        "\n",
        "accuracy=np.zeros(nEpoch)\n",
        "\n",
        "trainRate=0.05 #pick a number less than 1\n",
        "\n",
        "#batch\n",
        "batch_size = 14\n",
        "num_samples = len(Xtrain)\n",
        "\n",
        "# Calculate the number of batches\n",
        "num_batches = num_samples // batch_size\n",
        "\n",
        "print(XTrain.shape)\n",
        "\n",
        "# Split Xtrain and ytrain into batches\n",
        "batches = [(Xtrain[i * batch_size: (i + 1) * batch_size], ytrain[i * batch_size: (i + 1) * batch_size]) for i in range(num_batches)]\n",
        "\n",
        "\n",
        "#print(batches[1].shape)\n",
        "for iTrain in range(nEpoch):\n",
        "  #evaluate\n",
        "  #CALCULATE a\n",
        "  for batch in batches:\n",
        "     z = np.matmul(w.transpose(),batch)+b\n",
        "     a = sigmoid(z)\n",
        "\n",
        "    #calculate loss & accuracy\n",
        "     loss = avrLoss(yt,a)\n",
        "    #print(\"Loss: \", loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Xt = Xtrain[:,:]\n",
        "  #yt = ytrain[:,:]\n",
        "\n",
        "  #z = np.matmul(w.transpose(),Xt)+b\n",
        "    #print(\"z:\", z.shape)\n",
        "#  a = sigmoid(z)\n",
        "    #print(\"a\", a)\n",
        "\n",
        "    #print(\"y-train shape:\",ytrain.shape)\n",
        "    #y = ytrain[1,:]\n",
        "\n",
        "    #calculate loss & accuracy\n",
        "  loss = avrLoss(yt,a)\n",
        "    #print(\"Loss: \", loss)\n",
        "\n",
        "  yhat=a>.5\n",
        "  accuracy[iTrain]=np.mean(yhat==ytrain)\n",
        "\n",
        "    #update weights\n",
        "  wgrad = (a-yt)*Xt[i]\n",
        "  bgrad = a-yt\n",
        "\n",
        "    #print(\"grad:\")\n",
        "  print(\"wgrad: \", wgrad.shape)\n",
        "  print(w.shape)\n",
        "\n",
        "  assert(wgrad.shape == w.shape)\n",
        "\n",
        "    #UPDATE W AND b\n",
        "  ww = (w[i]-trainRate*wgrad)\n",
        "  w[i] = ww\n",
        "    #print(ww)\n",
        "  b = bgrad-trainRate*bgrad\n",
        "    #print(\"loss: \",loss)\n",
        "print(\"Epoch:\", iTrain, \"   Loss: \", loss)\n",
        "print(wgrad)\n",
        "\n",
        "if np.mod(iTrain,10)==0:\n",
        "  print(\"ITrian: \", iTrain,loss,accuracy[iTrain])\n",
        "\n",
        "Y_prediction_test = sigmoid(np.matmul(Xtest,w)+b)>0.5\n",
        "Y_prediction_train = sigmoid(np.matmul(Xtrain,w)+b)>0.5\n",
        "\n",
        "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - ytrain)) * 100))\n",
        "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - ytest)) * 100))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-74a5e349783b>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Split Xtrain and ytrain into batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'XTrain' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JmScwp3HJkr"
      },
      "source": [
        "#see what the network does:\n",
        "\n",
        "img = w.reshape(28,28)\n",
        "h=plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
        "plt.colorbar(h)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhIt3afDHN8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "80344081-627e-4720-aa12-1c24d626d1dd"
      },
      "source": [
        "#commpare with scikit-learn:\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(random_state=0).fit(Xtrain, ytrain)\n",
        "\n",
        "clf.predict(X)\n",
        "print('Train accuracy: ',clf.score(Xtrain, ytrain))\n",
        "print('Test accuracy: ',clf.score(Xtest, ytest))\n",
        "\n",
        "img = (clf.coef_).reshape(28,28)\n",
        "plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-216-6fedd0818f01>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0;34m\"This solver needs samples of at least 2 classes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
          ]
        }
      ]
    }
  ]
}